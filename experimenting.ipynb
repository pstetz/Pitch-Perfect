{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scipy imports\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# custom classes\n",
    "%run classes/measure.py\n",
    "%run classes/note.py\n",
    "\n",
    "def closest_duration(duration):\n",
    "        durations = np.array(list(duration_to_notes.keys()))\n",
    "        idx = (np.abs(durations - duration)).argmin()\n",
    "        return durations[idx]\n",
    "\n",
    "class Music:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 time_signature=(4, 4),\n",
    "                 tempo=120,\n",
    "                 ver_number=\"0.00\"):\n",
    "            \n",
    "        self.time_signature = time_signature\n",
    "        self.tempo = tempo\n",
    "        self.unit = 60 / (tempo * 4) # Finest resolution is 16th notes\n",
    "        self.unit_duration = tempo / 60\n",
    "        self.ver_number = ver_number # version number of decoder\n",
    "        \n",
    "    def find_peaks(self, sound, separation, min_volume_level):\n",
    "\n",
    "        # return value of peak positions and signal strength\n",
    "        peaks = list()\n",
    "\n",
    "        # initializing variables\n",
    "        max_prev_i = np.argmax(sound[:separation])\n",
    "        max_next_i = np.argmax(sound[separation + 1: 2 * separation]) + separation + 1\n",
    "        max_prev   = sound[max_prev_i]\n",
    "        max_next   = sound[max_next_i]\n",
    "\n",
    "        for i in range(separation, len(sound) - separation - 1):\n",
    "\n",
    "            # Determining the maximum value in the previous window\n",
    "            if sound[i - 1] > max_prev:\n",
    "                max_prev_i = i - 1\n",
    "                max_prev   = sound[max_prev_i]\n",
    "            elif i - max_prev_i > separation:\n",
    "                max_prev_i = np.argmax(sound[i - separation: i - 1]) + i - separation\n",
    "                max_prev   = sound[max_prev_i]\n",
    "\n",
    "            # Determining the maximum value in the next window\n",
    "            if sound[i + separation + 1] > max_next:\n",
    "                max_next_i = i + separation + 1\n",
    "                max_next   = sound[max_next_i]\n",
    "            elif max_next_i == i:\n",
    "                max_next_i = np.argmax(sound[i + 1: i + separation + 1]) + i + 1\n",
    "                max_next = sound[max_next_i]\n",
    "\n",
    "            # Determining if the current point is a peak\n",
    "            if sound[i] > max_prev and sound[i] > max_next and sound[i] > min_volume_level:\n",
    "                if len(peaks) == 0 or i - peaks[-1][0] > separation:\n",
    "                    peaks.append((i, sound[i]))\n",
    "        return peaks\n",
    "\n",
    "    def read(self, input_path, is_wav_format=True):\n",
    "        self.input_path = input_path\n",
    "        if is_wav_format:\n",
    "            self.sample_rate, self.raw = wavfile.read(input_path)\n",
    "        self.chan1, self.chan2 = list(map(list, zip(*self.raw)))\n",
    "        self.duration = len(self.raw) / self.sample_rate\n",
    "        \n",
    "    def get_input_path(self):\n",
    "        return self.input_path\n",
    "        \n",
    "    def compile_music(self, separation=3000, min_volume_level=5000, max_pitch=4000, stength_cutoff=0.75, use_chan1=True):\n",
    "        self.measures = list()\n",
    "        \n",
    "        if use_chan1:\n",
    "            peaks = self.find_peaks(self.chan1, separation, min_volume_level)\n",
    "            notes = self.get_notes(self.chan1, peaks, separation, max_pitch, stength_cutoff)\n",
    "        notes = self.filter_groups(notes)\n",
    "        notes = self.filter_nearby_times(notes)\n",
    "        return notes\n",
    "    \n",
    "    def get_notes(self, sound, peaks, separation, max_pitch, stength_cutoff):\n",
    "        notes = list()\n",
    "        for peak, loudness in peaks:\n",
    "            \n",
    "            inspection_zone = sound[peak: peak + separation]\n",
    "            fft_data = np.abs(fft(inspection_zone))\n",
    "\n",
    "            conversion_factor = self.sample_rate / len(fft_data)\n",
    "            max_signal = max(fft_data)\n",
    "            resonant_freqs = (-fft_data).argsort()\n",
    "            timestamp = peak / self.sample_rate\n",
    "\n",
    "            for freq in resonant_freqs:\n",
    "                signal = fft_data[freq]\n",
    "                if signal < stength_cutoff * max_signal:\n",
    "                    break\n",
    "                if freq * conversion_factor < max_pitch:\n",
    "                    note = Note(freq * conversion_factor, signal, loudness, timestamp)\n",
    "                    notes.append(note.getInfo())\n",
    "        notes = pd.DataFrame(notes, columns=[\"time\", \"id\", \"signal\", \"pitch\", \"given_pitch\",\n",
    "                                             \"loudness\", \"note\", \"octave\", \"alter\"])\n",
    "        return notes\n",
    "\n",
    "    # picks the loudest frequency for a certain time\n",
    "    def filter_groups(self, notes):\n",
    "        ret = pd.DataFrame(columns=notes.columns)\n",
    "        groups = notes.groupby(\"time\")\n",
    "\n",
    "        for key, note in groups:\n",
    "            \n",
    "            if len(note) == 1:\n",
    "                ret = ret.append(note)\n",
    "            else:\n",
    "                to_delete = list()\n",
    "                index_offset = min(note.index)\n",
    "                for i in range(index_offset, len(note) + index_offset):\n",
    "                    for j in range(i + 1, len(note) + index_offset):\n",
    "#                         if abs(note.id[i] - note.id[j]) < 2:\n",
    "                        to_delete.append(i if note.loudness[i] < note.loudness[j] else j)\n",
    "                ret = ret.append(note.drop(to_delete))\n",
    "        return ret\n",
    "    \n",
    "    # checks nearby notes for validation\n",
    "    def filter_nearby_times(self, notes):\n",
    "        self.start_offset = notes.iloc[0].time\n",
    "        notes[\"time\"]     = notes[\"time\"] - self.start_offset\n",
    "        notes[\"duration\"] = notes.time.shift(-1) - notes.time\n",
    "        notes[\"duration\"] = notes.duration.map(closest_duration)\n",
    "        notes[\"typ\"]      = notes.duration.map(lambda x: duration_to_notes[x][\"name\"])\n",
    "        return notes\n",
    "    \n",
    "    def format_notes(self, notes):\n",
    "        measure_counter, time_counter = 0, 0\n",
    "        curr_measure = Measure(measure_counter, self.time_signature[0], self.time_signature[1])\n",
    "        for i in range(len(notes)):\n",
    "            row = notes.iloc[i]\n",
    "            note = Note(row.given_pitch, row.signal, row.loudness, row.time, duration=row.duration, typ=row.typ)\n",
    "            if time_counter + row.duration > 4:\n",
    "                # FIXME: this fills up the rest of the measure with a rest, but it can be better\n",
    "                # Ideally it would be smart enough to wrap up a measure if there's little cutoff\n",
    "                # or tie current note into next measure.\n",
    "                curr_measure.wrap_up_time()\n",
    "                self.addMeasure(curr_measure)\n",
    "                measure_counter += 1\n",
    "                time_counter = 0\n",
    "                curr_measure = Measure(measure_counter, self.time_signature[0], self.time_signature[1])\n",
    "            curr_measure.addNote(row)\n",
    "        curr_measure.wrap_up_time()\n",
    "        self.addMeasure(curr_measure)\n",
    "            \n",
    "        \n",
    "    def addMeasure(self, measure):\n",
    "        self.measures.append(measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = Music()\n",
    "music.read('sounds/wav/cello_pluck/multi/a3_d3.wav')\n",
    "notes = music.compile_music()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "music.format_notes(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                     0\n",
       "id                      45\n",
       "signal         1.65916e+07\n",
       "pitch                  208\n",
       "given_pitch         207.65\n",
       "loudness             22888\n",
       "note                     G\n",
       "octave                   3\n",
       "alter                    1\n",
       "duration              0.25\n",
       "typ              sixteenth\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music.measures[0].notes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
